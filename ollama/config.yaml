name: "Ollama (Local LLM)"
version: "0.2.1"
slug: "ollama"
description: "Run Ollama locally for private, high-performance AI. Includes hardware-aware model recommendations."
url: "https://github.com/matthijsberg/HA-AI-Addons"
startup: application
boot: auto
arch:
  - aarch64
  - amd64
ports:
  11434/tcp: 11434
map:
  - config:rw
  - share:rw
options:
  model: "llama3:8b"
  custom_model: ""
schema:
  model: "list(llama3.2:3b|llama3.2:1b|llama3.1:8b|llama3.1:70b|phi3.5:3.8b|gemma2:2b|gemma2:9b|gemma2:27b|mistral:7b|qwen2.5:0.5b|qwen2.5:1.5b|qwen2.5:7b|qwen2.5:32b|qwen2.5:72b|deepseek-coder:1.3b|deepseek-coder:6.7b|codellama:7b|llava:7b|moondream:1.8b|tinyllama|starcoder2:3b|starcoder2:7b)"
  custom_model: "str?"
image: "ollama/ollama"
